{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# required library imports & variable declarations\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "# set seed value for reproducibility\n",
    "RANDOM_SEED = 1024"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading Cleaned Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>headline</th>\n",
       "      <th>headline_preprocessed</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Medicare Supplemental Policies: Do You Need One?</td>\n",
       "      <td>medicare supplemental policies do you need one</td>\n",
       "      <td>WELLNESS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7 Tips For You And Your Dog This July 4th</td>\n",
       "      <td>7 tips for you and your dog this july 4th</td>\n",
       "      <td>GREEN &amp; ENVIRONMENT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The Best Hotel-Hosted Super Bowl Parties In La...</td>\n",
       "      <td>the best hotelhosted super bowl parties in las...</td>\n",
       "      <td>TRAVEL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Even If You Lose The Weight, Obesity May Still...</td>\n",
       "      <td>even if you lose the weight obesity may still ...</td>\n",
       "      <td>HEALTHY LIVING</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Cocaine Cowboy 'White Boy Rick' Could Be Relea...</td>\n",
       "      <td>cocaine cowboy white boy rick could be release...</td>\n",
       "      <td>CRIME</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            headline  \\\n",
       "0   Medicare Supplemental Policies: Do You Need One?   \n",
       "1          7 Tips For You And Your Dog This July 4th   \n",
       "2  The Best Hotel-Hosted Super Bowl Parties In La...   \n",
       "3  Even If You Lose The Weight, Obesity May Still...   \n",
       "4  Cocaine Cowboy 'White Boy Rick' Could Be Relea...   \n",
       "\n",
       "                               headline_preprocessed             category  \n",
       "0     medicare supplemental policies do you need one             WELLNESS  \n",
       "1          7 tips for you and your dog this july 4th  GREEN & ENVIRONMENT  \n",
       "2  the best hotelhosted super bowl parties in las...               TRAVEL  \n",
       "3  even if you lose the weight obesity may still ...       HEALTHY LIVING  \n",
       "4  cocaine cowboy white boy rick could be release...                CRIME  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "# define relative data path (according the current path of this notebook) and data file name\n",
    "DATA_PATH = './scripts/data'\n",
    "\n",
    "df_train_full = pd.read_csv(f'{DATA_PATH}/train_cleaned.csv.gz')\n",
    "df_test_full  = pd.read_csv(f'{DATA_PATH}/test_cleaned.csv.gz')\n",
    "\n",
    "df_train_full.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subsampling Data\n",
    "Our dataset is large (159k+ rows for train and 40k+ for test portions). Needless to say, creating multiple models and tuning them will take a long time. What should we do then?<br>\n",
    "One simple trick I use in similar cases is to create a representative subsample dataset with smaller number of samples and then do the tuning magics on this one. Later on I can utilize the parameters I discovered and use them on full train data or entire dataset. Although this is not an ideal practice, it provides a good starting point when resources are limited."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's shuffle the whole dataframe before subsampling\n",
    "df_train_full = df_train_full.sample(frac=1, random_state=RANDOM_SEED).reset_index(drop=True)\n",
    "\n",
    "# second shuffle with an exponential random seed :D\n",
    "df_train_full = df_train_full.sample(frac=1, random_state=(RANDOM_SEED**2)).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make sure that the subsampled data is representative and similar to our original full dataset, it is advised widely to use __stratified__ method in all sampling scenarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "column_target = 'category'\n",
    "desired_subsample_size_train = 24000\n",
    "desired_subsample_size_valid = 6000\n",
    "\n",
    "# split the data into representative training and validation sets\n",
    "df_train_subsample, df_valid_subsample = train_test_split(df_train_full,\n",
    "                                                          train_size=desired_subsample_size_train,\n",
    "                                                          test_size=desired_subsample_size_valid,\n",
    "                                                          stratify=df_train_full[column_target],\n",
    "                                                          random_state=RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Labels:\n",
      "['PARENTING', 'GREEN & ENVIRONMENT', 'WORLD NEWS', 'WOMEN', 'ENTERTAINMENT', 'COMEDY', 'STYLE & BEAUTY', 'FOOD & DRINK', 'WELLNESS', 'COLLEGE', 'HEALTHY LIVING', 'TRAVEL', 'POLITICS', 'TECH', 'IMPACT', 'HOME & LIVING', 'BUSINESS', 'CRIME', 'MEDIA', 'BLACK VOICES', 'WEDDINGS', 'QUEER VOICES', 'MONEY', 'SPORTS', 'ARTS & CULTURE', 'LATINO VOICES', 'SCIENCE', 'GOOD NEWS', 'DIVORCE', 'WEIRD NEWS', 'RELIGION', 'FIFTY', 'EDUCATION']\n"
     ]
    }
   ],
   "source": [
    "category_map = dict(zip(df_train_full['category'].unique(), range(len(df_train_full['category'].unique()))))\n",
    "\n",
    "class_labels = list(category_map.keys())\n",
    "print(f'Classification Labels:\\n{class_labels}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Language Model Class\n",
    "Just like convolutional neural networks that enable feature extraction from images, language models like BERT, Roberta, etc. allow us to get contextual embedding or vector representation of an input text by the power of transformers and attention masks, which are pretty sophisticated neural network architecture designs. Our final model will utilize a language model inside. First we get the vector representation of the input from language model, and then pass it to the final layers of neural network. In fact, model training phase consist of fine-tuning the weights of our model's additional hidden layers for our classification task.\n",
    "\n",
    "But before getting into building our neural network, we need to create a class that will allow us to handle multiple language models in an easy way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Union\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "\n",
    "class LanguageModel():\n",
    "    def __init__(self, model_name: str, device: str, tokenizer_max_length: int=None):\n",
    "        self.model = AutoModel.from_pretrained(model_name)\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.tokenizer_max_length = tokenizer_max_length\n",
    "\n",
    "        # model initialization\n",
    "        self.model.to(device)\n",
    "        self.model.eval()\n",
    "\n",
    "\n",
    "    def get_embedding_size(self) -> int:\n",
    "        return self.model.config.hidden_size\n",
    "\n",
    "    def set_tokenizer_max_length(self, length: int) -> None:\n",
    "        self.tokenizer_max_length = length\n",
    "\n",
    "    def estimate_tokenizer_max_length(self, df: pd.DataFrame, text_column_name: str, estimation_type: str = None, adjustment_coefficient: float = 1.0) -> Union[int, dict]:\n",
    "        '''\n",
    "        Extracts the token length needed for language model based on desired estimation approach.\n",
    "        Uses a text column of a dataframe for the estimation of the token length.\n",
    "\n",
    "        Args:\n",
    "            df (pd.DataFrame): the dataframe to be used for the estimation\n",
    "            text_column_name (str): column title of the text column in the dataframe\n",
    "            estimation_type (str, optional): 'avg', 'max' or 'min' :: defaults to None\n",
    "            adjustment_coefficient (float): a coefficient to multiply the estimated token length by, defaults to 1.0 :: e.g. 1.2 => 20% larger token length\n",
    "\n",
    "        Returns:\n",
    "            int or dict: the estimated token length(s)\n",
    "        '''\n",
    "        if adjustment_coefficient <= 0.0:\n",
    "            raise ValueError(\"adjust_amount must be a positive float.\")\n",
    "\n",
    "        if estimation_type is not None:\n",
    "            self.validate_tokenizer_max_length(estimation_type, identifier='estimation_type')\n",
    "\n",
    "        results = dict()\n",
    "        # find the rows with the most and least count of words\n",
    "        word_counts = df[text_column_name].str.split().str.len()\n",
    "        index_max = word_counts.argmax()\n",
    "        index_min = word_counts.argmin()\n",
    "        text_with_max_words = df[text_column_name].iloc[index_max]\n",
    "        text_with_min_words = df[text_column_name].iloc[index_min]\n",
    "        # tokenize them and add special tokens, for example `[CLS]` and `[SEP]`\n",
    "        token_ids_most  = self.tokenizer.encode(text_with_max_words, add_special_tokens=True)\n",
    "        token_ids_least = self.tokenizer.encode(text_with_min_words, add_special_tokens=True)\n",
    "        # push 'max' and 'min' token ids length to results list\n",
    "        results['max'] = round(len(token_ids_most) * adjustment_coefficient)\n",
    "        results['min'] = round(len(token_ids_least) * adjustment_coefficient)\n",
    "\n",
    "        # do not continue any further if we don't need to estimate 'avg'\n",
    "        if estimation_type in ['max', 'min']:\n",
    "            return results[estimation_type]\n",
    "\n",
    "        # 'avg' or altogether\n",
    "        tokens_sum = 0\n",
    "        all_text = df[text_column_name]\n",
    "        for text in all_text:\n",
    "            # tokenize the text and add special tokens, for example `[CLS]` and `[SEP]`\n",
    "            token_ids = self.tokenizer.encode(text, add_special_tokens=True)\n",
    "            # update the token length sum\n",
    "            tokens_sum += len(token_ids)\n",
    "        results['avg'] = round(round(tokens_sum / len(df)) * adjustment_coefficient)\n",
    "\n",
    "        return results['avg'] if estimation_type == 'avg' else results\n",
    "\n",
    "    def validate_tokenizer_max_length(self, token_length: Union[int, str], identifier='token_length') -> bool:\n",
    "        if (not isinstance(token_length, int) and not isinstance(token_length, str)) or \\\n",
    "        (not isinstance(token_length, str) and not isinstance(token_length, int)):\n",
    "            raise ValueError(f\"Wrong value provided, please check '{identifier}' parameter.\")\n",
    "        if isinstance(token_length, str) and token_length not in ['avg', 'max', 'min']:\n",
    "            raise ValueError(f\"Wrong value provided, please check '{identifier}' parameter.\\n\\\n",
    "                            It must be one of 'avg', 'max' or 'min' values.\")\n",
    "        if isinstance(token_length, int) and token_length < 1:\n",
    "            raise ValueError(f\"Wrong value provided, please check '{identifier}' parameter.\\n\\\n",
    "                            It must be a positive integer.\")\n",
    "        return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building PyTorch Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch runs on =>  cuda:0 \n",
      "PyTorch version =>  1.10.0+cu113\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import Dataset, DataLoader, Sampler, WeightedRandomSampler\n",
    "\n",
    "\n",
    "# GPU support\n",
    "use_gpu = True\n",
    "cuda_available = torch.cuda.is_available()\n",
    "device = torch.device('cuda:0' if (cuda_available and use_gpu) else 'cpu')\n",
    "print('PyTorch runs on => ', device, '\\nPyTorch version => ', torch.__version__)\n",
    "if use_gpu:\n",
    "    torch.cuda.empty_cache()\n",
    "torch.set_num_threads(torch.get_num_threads()-1)\n",
    "\n",
    "# reproducibility (almost)\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "torch.cuda.manual_seed(RANDOM_SEED)\n",
    "torch.cuda.manual_seed_all(RANDOM_SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "\n",
    "\n",
    "# pytorch dataset class\n",
    "class TextClassificationDataset(Dataset):\n",
    "    def __init__(self, df: pd.DataFrame, text_column_name: str, label_column_name: str, language_model: LanguageModel, preprocess: bool = False):\n",
    "        self.df = df\n",
    "        self.tokenizer = language_model.tokenizer\n",
    "        self.tokenizer_max_length = language_model.tokenizer_max_length\n",
    "        if preprocess:\n",
    "            text_column = f'{text_column_name}_preprocessed'\n",
    "            self.df[text_column] = df.apply(lambda row: self.preprocess_text(str(row[text_column_name])), axis=1)\n",
    "        else: text_column = label_column_name\n",
    "        self.titles  = df[text_column].to_numpy()\n",
    "        self.targets = df[label_column_name].to_numpy()\n",
    "        self.preprocess_enabled = preprocess\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.titles[idx])\n",
    "        label = self.targets[idx]\n",
    "        text_encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.tokenizer_max_length,\n",
    "            return_token_type_ids=False,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt',\n",
    "        )\n",
    "        sample = {\n",
    "            'token_ids': text_encoding['input_ids'].flatten(),\n",
    "            'attention_mask': text_encoding['attention_mask'].flatten()}\n",
    "        output = torch.tensor(label, dtype=torch.long)\n",
    "        return (sample, output)\n",
    "\n",
    "    def preprocess_text(self, text: str) -> str:\n",
    "        text = text.lower()  # convert to lowercase\n",
    "        text = re.sub('(#)(\\S+)', r' \\2', text)  # remove hashtags sign\n",
    "        text = re.sub('[%s]' % re.escape(string.punctuation), '', text)  # remove punctuations\n",
    "        text = re.sub(' +', ' ', text)  # replace multiple whitespaces with a single space\n",
    "        text = text.strip()  # remove leading and trailing whitespaces\n",
    "        return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataloader Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataloader class\n",
    "class TextClassificationDataLoader(DataLoader):\n",
    "    def __init__(self, dataset: Dataset, batch_size: int = 128, shuffle: bool = False, sampler: Sampler = None, num_workers: int = 0):\n",
    "        super().__init__(dataset, batch_size=batch_size, shuffle=shuffle, sampler=sampler, num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "97ae724bfa85b9b34df7982b8bb8c7216f435b92902d749e4263f71162bea840"
  },
  "kernelspec": {
   "display_name": "Python 3.7.12 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
