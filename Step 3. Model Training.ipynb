{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# required library imports & variable declarations\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "# set seed value for reproducibility\n",
    "RANDOM_SEED = 1024"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading Cleaned Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>headline</th>\n",
       "      <th>headline_preprocessed</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Medicare Supplemental Policies: Do You Need One?</td>\n",
       "      <td>medicare supplemental policies do you need one</td>\n",
       "      <td>WELLNESS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7 Tips For You And Your Dog This July 4th</td>\n",
       "      <td>7 tips for you and your dog this july 4th</td>\n",
       "      <td>GREEN &amp; ENVIRONMENT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The Best Hotel-Hosted Super Bowl Parties In La...</td>\n",
       "      <td>the best hotelhosted super bowl parties in las...</td>\n",
       "      <td>TRAVEL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Even If You Lose The Weight, Obesity May Still...</td>\n",
       "      <td>even if you lose the weight obesity may still ...</td>\n",
       "      <td>HEALTHY LIVING</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Cocaine Cowboy 'White Boy Rick' Could Be Relea...</td>\n",
       "      <td>cocaine cowboy white boy rick could be release...</td>\n",
       "      <td>CRIME</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            headline  \\\n",
       "0   Medicare Supplemental Policies: Do You Need One?   \n",
       "1          7 Tips For You And Your Dog This July 4th   \n",
       "2  The Best Hotel-Hosted Super Bowl Parties In La...   \n",
       "3  Even If You Lose The Weight, Obesity May Still...   \n",
       "4  Cocaine Cowboy 'White Boy Rick' Could Be Relea...   \n",
       "\n",
       "                               headline_preprocessed             category  \n",
       "0     medicare supplemental policies do you need one             WELLNESS  \n",
       "1          7 tips for you and your dog this july 4th  GREEN & ENVIRONMENT  \n",
       "2  the best hotelhosted super bowl parties in las...               TRAVEL  \n",
       "3  even if you lose the weight obesity may still ...       HEALTHY LIVING  \n",
       "4  cocaine cowboy white boy rick could be release...                CRIME  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "# define relative data path (according the current path of this notebook) and data file name\n",
    "DATA_PATH = './scripts/data'\n",
    "\n",
    "df_train_full = pd.read_csv(f'{DATA_PATH}/train_cleaned.csv.gz')\n",
    "df_test_full  = pd.read_csv(f'{DATA_PATH}/test_cleaned.csv.gz')\n",
    "\n",
    "df_train_full.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subsampling Data\n",
    "Our dataset is large (159k+ rows for train and 40k+ for test portions). Needless to say, creating multiple models and tuning them will take a long time. What should we do then?<br>\n",
    "One simple trick I use in similar cases is to create a representative subsample dataset with smaller number of samples and then do the tuning magics on this one. Later on I can utilize the parameters I discovered and use them on full train data or entire dataset. Although this is not an ideal practice, it provides a good starting point when resources are limited."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's shuffle the whole dataframe before subsampling\n",
    "df_train_full = df_train_full.sample(frac=1, random_state=RANDOM_SEED).reset_index(drop=True)\n",
    "\n",
    "# second shuffle with an exponential random seed :D\n",
    "df_train_full = df_train_full.sample(frac=1, random_state=(RANDOM_SEED**2)).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make sure that the subsampled data is representative and similar to our original full dataset, it is advised widely to use __stratified__ method in all sampling scenarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "column_target = 'category'\n",
    "desired_subsample_size_train = 8000\n",
    "desired_subsample_size_valid = 8000\n",
    "\n",
    "# split the data into representative training and validation sets\n",
    "df_train_subsample, df_valid_subsample = train_test_split(\n",
    "    df_train_full,\n",
    "    train_size=desired_subsample_size_train,\n",
    "    test_size=desired_subsample_size_valid,\n",
    "    stratify=df_train_full[column_target],\n",
    "    random_state=RANDOM_SEED\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Labels:\n",
      "['PARENTING', 'GREEN & ENVIRONMENT', 'WORLD NEWS', 'WOMEN', 'ENTERTAINMENT', 'COMEDY', 'STYLE & BEAUTY', 'FOOD & DRINK', 'WELLNESS', 'COLLEGE', 'HEALTHY LIVING', 'TRAVEL', 'POLITICS', 'TECH', 'IMPACT', 'HOME & LIVING', 'BUSINESS', 'CRIME', 'MEDIA', 'BLACK VOICES', 'WEDDINGS', 'QUEER VOICES', 'MONEY', 'SPORTS', 'ARTS & CULTURE', 'LATINO VOICES', 'SCIENCE', 'GOOD NEWS', 'DIVORCE', 'WEIRD NEWS', 'RELIGION', 'FIFTY', 'EDUCATION']\n"
     ]
    }
   ],
   "source": [
    "category_map = dict(zip(df_train_full['category'].unique(), range(len(df_train_full['category'].unique()))))\n",
    "\n",
    "class_labels = list(category_map.keys())\n",
    "print(f'Classification Labels:\\n{class_labels}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Language Model Class\n",
    "Just like convolutional neural networks that enable feature extraction from images, language models like BERT, Roberta, etc. allow us to get contextual embedding or vector representation of an input text by the power of transformers and attention masks, which are pretty sophisticated neural network architecture designs. Our final model will utilize a language model inside. First we get the vector representation of the input from language model, and then pass it to the final layers of neural network. In fact, model training phase consist of fine-tuning the weights of our model's additional hidden layers for our classification task.\n",
    "\n",
    "But before getting into building our neural network, we need to create a class that will allow us to handle multiple language models in an easy way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Union\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "\n",
    "class LanguageModel():\n",
    "    def __init__(self, model_name: str, device: str, tokenizer_max_length: int=None, fine_tune_model: bool=False):\n",
    "        self.model = AutoModel.from_pretrained(model_name, return_dict=False)\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name, return_dict=False)\n",
    "        self.tokenizer_max_length = tokenizer_max_length\n",
    "\n",
    "        # model initialization\n",
    "        self.model.to(device)\n",
    "        if fine_tune_model:\n",
    "            self.model.train()\n",
    "        else:\n",
    "            self.model.eval()\n",
    "\n",
    "\n",
    "    def get_embedding_size(self) -> int:\n",
    "        return self.model.config.hidden_size\n",
    "\n",
    "    def set_tokenizer_max_length(self, length: int) -> None:\n",
    "        self.tokenizer_max_length = length\n",
    "\n",
    "    def estimate_tokenizer_max_length(self, df: pd.DataFrame, text_column_name: str, estimation_type: str = None, adjustment_coefficient: float = 1.0) -> Union[int, dict]:\n",
    "        '''\n",
    "        Extracts the token length needed for language model based on desired estimation approach.\n",
    "        Uses a text column of a dataframe for the estimation of the token length.\n",
    "\n",
    "        Args:\n",
    "            df (pd.DataFrame): the dataframe to be used for the estimation\n",
    "            text_column_name (str): column title of the text column in the dataframe\n",
    "            estimation_type (str, optional): 'avg', 'max' or 'min' :: defaults to None\n",
    "            adjustment_coefficient (float): a coefficient to multiply the estimated token length by, defaults to 1.0 :: e.g. 1.2 => 20% larger token length\n",
    "\n",
    "        Returns:\n",
    "            int or dict: the estimated token length(s)\n",
    "        '''\n",
    "        if adjustment_coefficient <= 0.0:\n",
    "            raise ValueError(\"adjust_amount must be a positive float.\")\n",
    "\n",
    "        if estimation_type is not None:\n",
    "            self.validate_tokenizer_max_length(estimation_type, identifier='estimation_type')\n",
    "\n",
    "        results = dict()\n",
    "        # find the rows with the most and least count of words\n",
    "        word_counts = df[text_column_name].str.split().str.len()\n",
    "        index_max = word_counts.argmax()\n",
    "        index_min = word_counts.argmin()\n",
    "        text_with_max_words = df[text_column_name].iloc[index_max]\n",
    "        text_with_min_words = df[text_column_name].iloc[index_min]\n",
    "        # tokenize them and add special tokens, for example `[CLS]` and `[SEP]`\n",
    "        token_ids_most  = self.tokenizer.encode(text_with_max_words, add_special_tokens=True)\n",
    "        token_ids_least = self.tokenizer.encode(text_with_min_words, add_special_tokens=True)\n",
    "        # push 'max' and 'min' token ids length to results list\n",
    "        results['max'] = round(len(token_ids_most) * adjustment_coefficient)\n",
    "        results['min'] = round(len(token_ids_least) * adjustment_coefficient)\n",
    "\n",
    "        # do not continue any further if we don't need to estimate 'avg'\n",
    "        if estimation_type in ['max', 'min']:\n",
    "            return results[estimation_type]\n",
    "\n",
    "        # 'avg' or altogether\n",
    "        tokens_sum = 0\n",
    "        all_text = df[text_column_name]\n",
    "        for text in all_text:\n",
    "            # tokenize the text and add special tokens, for example `[CLS]` and `[SEP]`\n",
    "            token_ids = self.tokenizer.encode(text, add_special_tokens=True)\n",
    "            # update the token length sum\n",
    "            tokens_sum += len(token_ids)\n",
    "        results['avg'] = round(round(tokens_sum / len(df)) * adjustment_coefficient)\n",
    "\n",
    "        return results['avg'] if estimation_type == 'avg' else results\n",
    "\n",
    "    def validate_tokenizer_max_length(self, token_length: Union[int, str], identifier='token_length') -> bool:\n",
    "        if (not isinstance(token_length, int) and not isinstance(token_length, str)) or \\\n",
    "        (not isinstance(token_length, str) and not isinstance(token_length, int)):\n",
    "            raise ValueError(f\"Wrong value provided, please check '{identifier}' parameter.\")\n",
    "        if isinstance(token_length, str) and token_length not in ['avg', 'max', 'min']:\n",
    "            raise ValueError(f\"Wrong value provided, please check '{identifier}' parameter.\\n\\\n",
    "                            It must be one of 'avg', 'max' or 'min' values.\")\n",
    "        if isinstance(token_length, int) and token_length < 1:\n",
    "            raise ValueError(f\"Wrong value provided, please check '{identifier}' parameter.\\n\\\n",
    "                            It must be a positive integer.\")\n",
    "        return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building PyTorch Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch runs on =>  cuda:0 \n",
      "PyTorch version =>  1.10.0+cu113\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import Dataset, DataLoader, Sampler\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "\n",
    "# GPU support\n",
    "use_gpu = True\n",
    "cuda_available = torch.cuda.is_available()\n",
    "device = torch.device('cuda:0' if (cuda_available and use_gpu) else 'cpu')\n",
    "print('PyTorch runs on => ', device, '\\nPyTorch version => ', torch.__version__)\n",
    "if use_gpu:\n",
    "    torch.cuda.empty_cache()\n",
    "torch.set_num_threads(torch.get_num_threads()-1)\n",
    "\n",
    "# reproducibility (almost)\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "torch.cuda.manual_seed(RANDOM_SEED)\n",
    "torch.cuda.manual_seed_all(RANDOM_SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "\n",
    "\n",
    "# pytorch dataset class\n",
    "class TextClassificationDataset(Dataset):\n",
    "    def __init__(self, df: pd.DataFrame, text_column_name: str, label_column_name: str, language_model: LanguageModel, preprocess: bool = False):\n",
    "        self.df = df\n",
    "        self.tokenizer = language_model.tokenizer\n",
    "        self.tokenizer_max_length = language_model.tokenizer_max_length\n",
    "\n",
    "        if preprocess:\n",
    "            text_column = f'{text_column_name}_preprocessed'\n",
    "            self.df[text_column] = df.apply(lambda row: self.preprocess_text(str(row[text_column_name])), axis=1)\n",
    "        else: text_column = text_column_name\n",
    "\n",
    "        # get category name and corresponding index from the dataframe\n",
    "        classes_map  = dict(zip(df[label_column_name].unique(), range(len(df[label_column_name].unique()))))\n",
    "        self.titles  = df[text_column].to_numpy()\n",
    "        self.targets = df[label_column_name].map(classes_map).to_numpy()\n",
    "        self.preprocess_enabled = preprocess\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.titles[idx])\n",
    "        label = self.targets[idx]\n",
    "        text_encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.tokenizer_max_length,\n",
    "            return_token_type_ids=False,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt',\n",
    "        )\n",
    "        sample = {\n",
    "            'token_ids': text_encoding['input_ids'].flatten(),\n",
    "            'attention_mask': text_encoding['attention_mask'].flatten()}\n",
    "        output = torch.tensor(label, dtype=torch.long)\n",
    "        return (sample, output)\n",
    "\n",
    "    def preprocess_text(self, text: str) -> str:\n",
    "        text = text.lower()  # convert to lowercase\n",
    "        text = re.sub('(#)(\\S+)', r' \\2', text)  # remove hashtags sign\n",
    "        text = re.sub('[%s]' % re.escape(string.punctuation), '', text)  # remove punctuations\n",
    "        text = re.sub(' +', ' ', text)  # replace multiple whitespaces with a single space\n",
    "        text = text.strip()  # remove leading and trailing whitespaces\n",
    "        return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataloader Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pytorch dataloader class\n",
    "class TextClassificationDataLoader(DataLoader):\n",
    "    def __init__(self, dataset: Dataset, batch_size: int = 128, shuffle: bool = False, sampler: Sampler = None, num_workers: int = 0):\n",
    "        super().__init__(dataset, batch_size=batch_size, shuffle=shuffle, sampler=sampler, num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pytorch model class\n",
    "class TopicClassifier(nn.Module):\n",
    "    def __init__(self, n_classes: int, language_model: LanguageModel, nn_options: dict):\n",
    "        super(TopicClassifier, self).__init__()\n",
    "\n",
    "        self.n_classes = n_classes\n",
    "\n",
    "        self.lm = language_model.model\n",
    "        self.lm.eval()  # important: evaluation mode must be enabled for language model, to prevent re-training model on each call\n",
    "\n",
    "        self.emb_size = language_model.get_embedding_size()  # embedding size\n",
    "\n",
    "        self.embedding_normalization = nn_options['embedding_normalization']\n",
    "        if self.embedding_normalization['enabled']:\n",
    "            if self.embedding_normalization['type'] == 'simple':\n",
    "                self.emb_normalizer = nn.functional.normalize\n",
    "            elif self.embedding_normalization['type'] == 'standard':\n",
    "                self.emb_normalizer = nn.LayerNorm(self.emb_size)\n",
    "            else:\n",
    "                raise ValueError(f\"Invalid normalization type: {self.normalization['type']}\")\n",
    "\n",
    "        self.n_hidden_layers = nn_options['n_hidden_layers']\n",
    "        self.use_dropout = nn_options['use_dropout']\n",
    "        self.dropout_val = nn_options['dropout_val']\n",
    "        self.use_activation_fn = nn_options['use_activation_fn']\n",
    "        self.activation_fn = nn_options['activation_fn']\n",
    "\n",
    "        self.fc = nn.Sequential(\n",
    "            # hidden layer(s)\n",
    "            self.generate_hidden_unit(self.n_hidden_layers, self.emb_size),\n",
    "            # output layer\n",
    "            nn.Linear(self.emb_size, self.n_classes),\n",
    "        )\n",
    "\n",
    "    # generate hidden unit\n",
    "    def generate_hidden_unit(self, n_layers, layer_size):\n",
    "        layers = []\n",
    "        for i in range(n_layers):\n",
    "            layers.append(nn.Linear(layer_size, layer_size))\n",
    "            if self.use_activation_fn:\n",
    "                layers.append(self.activation_fn)\n",
    "            if self.use_dropout:\n",
    "                layers.append(nn.Dropout(self.dropout_val))\n",
    "        block = nn.Sequential(*layers)\n",
    "        return block\n",
    "\n",
    "    def forward(self, x):\n",
    "        _, embedding = self.lm(\n",
    "            input_ids=x['token_ids'],\n",
    "            attention_mask=x['attention_mask']\n",
    "        )\n",
    "        if self.embedding_normalization['enabled']: embedding = self.emb_normalizer(embedding)\n",
    "\n",
    "        output = self.fc(embedding)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "\n",
    "def train_step(model, data_loader, loss_fn, optimizer, device):\n",
    "    model.train()\n",
    "\n",
    "    losses = []\n",
    "    predictions = []\n",
    "    labels = []\n",
    "\n",
    "    for i, (train_inputs, train_labels) in enumerate(data_loader):\n",
    "        for input_tensor in train_inputs:\n",
    "            train_inputs[input_tensor] = train_inputs[input_tensor].to(device)\n",
    "        train_labels = train_labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model(train_inputs)\n",
    "\n",
    "        loss = loss_fn(output, train_labels)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.3)  # prevent exploding gradients\n",
    "        optimizer.step()\n",
    "\n",
    "        _, preds = torch.max(output, dim=1)\n",
    "        losses.append(loss.item())\n",
    "        predictions.append(preds.cpu().numpy())\n",
    "        labels.append(train_labels.data.cpu().numpy())\n",
    "\n",
    "    # calculate accuracy & f1 score\n",
    "    predictions = np.concatenate(predictions, axis=0)\n",
    "    labels = np.concatenate(labels, axis=0)\n",
    "    f1 = f1_score(labels, predictions, average='macro')\n",
    "    acc = accuracy_score(labels, predictions)\n",
    "\n",
    "    return np.mean(losses), acc, f1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(model, data_loader, loss_fn, device):\n",
    "    model.eval()\n",
    "\n",
    "    losses = []\n",
    "    predictions = []\n",
    "    labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, (test_inputs, test_labels) in enumerate(data_loader):\n",
    "            for input_tensor in test_inputs:\n",
    "                test_inputs[input_tensor] = test_inputs[input_tensor].to(device)\n",
    "            test_labels = test_labels.to(device)\n",
    "\n",
    "            output = model(test_inputs)\n",
    "            loss = loss_fn(output, test_labels)\n",
    "\n",
    "            _, preds = torch.max(output, dim=1)\n",
    "            losses.append(loss.item())\n",
    "            predictions.append(preds.cpu().numpy())\n",
    "            labels.append(test_labels.data.cpu().numpy())\n",
    "\n",
    "    # calculate accuracy & f1 score\n",
    "    predictions = np.concatenate(predictions, axis=0)\n",
    "    labels = np.concatenate(labels, axis=0)\n",
    "    f1 = f1_score(labels, predictions, average='macro')\n",
    "    acc = accuracy_score(labels, predictions)\n",
    "\n",
    "    return np.mean(losses), acc, f1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "def train(model, train_dataloader, validation_dataloader, loss_fn, optimizer, scheduler, device, n_epochs):\n",
    "    train_history = defaultdict(list)\n",
    "    best_epoch = 1\n",
    "    best_loss = float('inf')\n",
    "    best_loss_f1 = 0.0\n",
    "    best_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        print(f\"Epoch {epoch + 1}/{n_epochs}\")\n",
    "        print('-' * 10)\n",
    "\n",
    "        train_loss, train_acc, train_f1 = train_step(model, train_dataloader, loss_fn, optimizer, device)\n",
    "        print(f'Train Loss: {train_loss:.6f}, Accuracy: {train_acc:.4f}, F1: {train_f1:.4f}')\n",
    "\n",
    "        validation_loss, validation_acc, validation_f1 = eval_model(model, validation_dataloader, loss_fn, device)\n",
    "        print(f'Valid Loss: {validation_loss:.6f}, Accuracy: {validation_acc:.4f}, F1: {validation_f1:.4f}', end='\\n\\n')\n",
    "\n",
    "        train_history['train_loss'].append(train_loss)\n",
    "        train_history['train_acc'].append(train_acc)\n",
    "        train_history['train_f1'].append(train_f1)\n",
    "        train_history['val_loss'].append(validation_loss)\n",
    "        train_history['val_acc'].append(validation_acc)\n",
    "        train_history['val_f1'].append(validation_f1)\n",
    "\n",
    "        # scheduler.step(validation_loss)\n",
    "        scheduler.step()\n",
    "\n",
    "        # update best model stats\n",
    "        if validation_loss < best_loss:\n",
    "            best_loss = validation_loss\n",
    "            best_loss_f1 = validation_f1\n",
    "            best_wts = copy.deepcopy(model.state_dict())\n",
    "            best_epoch = (epoch + 1)\n",
    "\n",
    "    # load best model weights on model object after train is complete\n",
    "    model.load_state_dict(best_wts)\n",
    "    # print best model stats\n",
    "    print(f'best model criteria => least validation loss: {best_loss:.6f} @ epoch {best_epoch} with f1 score of: {best_loss_f1:.4f} ')\n",
    "    return train_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning & Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initializing Instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# initialize language model\n",
    "lm_BERT = LanguageModel(model_name='bert-base-uncased', device=device)\n",
    "\n",
    "# estimate max length of tokenizer\n",
    "tokenizer_len = lm_BERT.estimate_tokenizer_max_length(\n",
    "    df=df_train_subsample,\n",
    "    text_column_name='headline_preprocessed',\n",
    "    estimation_type='avg',\n",
    "    adjustment_coefficient=1.5\n",
    ")\n",
    "\n",
    "# set tokenizer max length\n",
    "lm_BERT.set_tokenizer_max_length(tokenizer_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize dataset & dataloader objects\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "# train dataset\n",
    "train_ds = TextClassificationDataset(\n",
    "    df=df_train_subsample,\n",
    "    text_column_name='headline_preprocessed',\n",
    "    label_column_name='category',\n",
    "    language_model=lm_BERT\n",
    ")\n",
    "\n",
    "# validation dataset\n",
    "valid_ds = TextClassificationDataset(\n",
    "    df=df_valid_subsample,\n",
    "    text_column_name='headline_preprocessed',\n",
    "    label_column_name='category',\n",
    "    language_model=lm_BERT\n",
    ")\n",
    "\n",
    "# train dataloader\n",
    "train_dl = TextClassificationDataLoader(\n",
    "    dataset=train_ds,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "# validation dataloader\n",
    "valid_dl = TextClassificationDataLoader(\n",
    "    dataset=valid_ds,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize model properties\n",
    "\n",
    "\n",
    "# neural network architecture parameters\n",
    "nn_options_dict = {\n",
    "    # number of hidden layers in the network (the larger the number, the more complex the model)\n",
    "    'n_hidden_layers': 2,\n",
    "    'embedding_normalization': {\n",
    "        'enabled': True,  # whether to normalize the input embedding vectors\n",
    "        'type': 'standard', # 'simple' | 'standard'  :: 'simple' => normalize to (0~1) range, 'standard' => normalize using mean and standard-deviation\n",
    "    },\n",
    "    'use_dropout': False,\n",
    "    'dropout_val': 0.2,\n",
    "    'use_activation_fn': True,\n",
    "    'activation_fn': nn.ReLU()    # nn.LeakyReLU(), nn.SELU(), ...\n",
    "}\n",
    "\n",
    "# neural network model\n",
    "model = TopicClassifier(n_classes=len(class_labels), language_model=lm_BERT, nn_options=nn_options_dict)\n",
    "model = model.to(device)\n",
    "\n",
    "\n",
    "# optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters())\n",
    "\n",
    "\n",
    "# loss function\n",
    "# use CrossEntropy loss given we have a multi-class classification problem\n",
    "# this loss function also automatically applies softmax to the output of the model\n",
    "loss_fn = nn.CrossEntropyLoss().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding Optimal Learning Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:23<00:00,  2.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate search finished. See the graph with {finder_name}.plot()\n",
      "LR suggestion: steepest gradient\n",
      "Suggested LR: 3.49E-04\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from torch_lr_finder import LRFinder\n",
    "\n",
    "lr_finder = LRFinder(model, optimizer, loss_fn, device=device)\n",
    "lr_finder.range_test(train_dl, start_lr=1e-5, end_lr=5e-3, num_iter=50)\n",
    "# lr_finder.range_test(train_loader=train_dl, val_loader=valid_dl, start_lr=1e-5, end_lr=1e-1, num_iter=50, step_mode=\"exp\")\n",
    "lr_finder.plot()  # to inspect the loss-learning rate graph\n",
    "lr_finder.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "NUM_EPOCHS = 10\n",
    "\n",
    "\n",
    "# optimizer with newly found optimum learning rate\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=3.5e-04)\n",
    "\n",
    "# scheduler to adjust learning rate during training\n",
    "total_steps = len(train_dl) * NUM_EPOCHS\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=total_steps\n",
    ")\n",
    "\n",
    "# train model\n",
    "train_history = train(\n",
    "    model=model,\n",
    "    train_dataloader=train_dl,\n",
    "    validation_dataloader=valid_dl,\n",
    "    loss_fn=loss_fn,\n",
    "    optimizer=optimizer,\n",
    "    scheduler=scheduler,\n",
    "    device=device,\n",
    "    n_epochs=NUM_EPOCHS,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "97ae724bfa85b9b34df7982b8bb8c7216f435b92902d749e4263f71162bea840"
  },
  "kernelspec": {
   "display_name": "Python 3.7.12 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
