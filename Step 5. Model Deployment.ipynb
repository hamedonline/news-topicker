{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Deployment\n",
    "Quite a journey to get here! Do you remember that at the end of step 3 (model training) we saved the trained model as a bin file?<br>\n",
    "Well, now it's time to serve the model to those folks that have been waiting for us to provide them an entrance for prediction ðŸ˜Š. __REST APIs__ are the most common method used for doing this.\n",
    "\n",
    "During this notebook, we'll be reviewing how to do so, in simplest terms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Web Service API\n",
    "Our prediction model needs to be wrapped into a web service, available to the outside world. When it comes to creating API web services, there are plenty of options. [Flask](https://palletsprojects.com/p/flask/) is a micro web application framework that makes creating web APIs like a breeze. [FastAPI](https://fastapi.tiangolo.com/) is another fantastic option.\n",
    "\n",
    "The API wrapper will do the below three tasks:\n",
    "- Receive inputs as a JSON string via POST method\n",
    "- Load prediction model from a saved file\n",
    "- Run prediction on input data and return prediction result response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are three functions in our prediction file.<br>\n",
    "- First one, __*load_model()*__, loads the model saved as a pickle file. As a side note, we know that our saved model is embodied with data preprocessing pipeline alongside.\n",
    "- Second function, __*get_prediction()*__, does the prediction task by utilizing a model and the input data. The __input data must be in the same dimension__ as our cleaned dataset, meaning it must have the exact features we used for training.\n",
    "- The last one, __*predict()*__ , is the entry for our API endpoint handled by Flask. Notice the __*route()*__ and it's input above function definition. The route parameter can be any name you like, which will be used by outside world as API's entry point.\n",
    "\n",
    "The last line employs _waitress_ web server to run the flask app. Waitress is a lightweight WSGI server that has no dependencies except ones which live in the Python standard library. It can run on both Windows & UNIX-based OS environments.\n",
    "The code above is available in ``scripts/predict.py`` file. We need to run this file in order to make our API accessible.\n",
    "\n",
    "All you need to do is to run a shell command inside the __*scripts/inference*__ folder to run the file. Type-in one of the following commands in your command line inside this folder:\n",
    "\n",
    "```shell\n",
    "\"python inference.py\"\n",
    "```\n",
    "\n",
    "or\n",
    "\n",
    "```shell\n",
    "\"python -m inference\"\n",
    "```\n",
    "\n",
    "That's it. Now our flask app is running in the background, waiting to serve requests on 9696 port, available on all IP addresses our machine has. Note that, in the last line containing the __*serve()*__ function in ``inference.py`` file, you can put-in any port number as function's port parameter, as long as it is not used by any apps or services running on your machine."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "97ae724bfa85b9b34df7982b8bb8c7216f435b92902d749e4263f71162bea840"
  },
  "kernelspec": {
   "display_name": "Python 3.7.12 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
